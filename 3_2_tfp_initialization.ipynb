{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "306430bb",
   "metadata": {},
   "source": [
    "In this script, we demonstrate how to initialize and change values in TFP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45ca37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e313ec5",
   "metadata": {},
   "source": [
    "Let's set up a Beta prior/posterior so that we have a clearly constrained posterior support of (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69dbf3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function build_stateful_trainable in module tensorflow_probability.python.internal.trainable_state_util:\n",
      "\n",
      "build_stateful_trainable(*args, seed=None, **kwargs)\n",
      "    Builds a joint variational posterior that factors over model variables.\n",
      "\n",
      "    By default, this method creates an independent trainable Normal distribution\n",
      "    for each variable, transformed using a bijector (if provided) to\n",
      "    match the support of that variable. This makes extremely strong\n",
      "    assumptions about the posterior: that it is approximately normal (or\n",
      "    transformed normal), and that all model variables are independent.\n",
      "\n",
      "    Args:\n",
      "      event_shape: `Tensor` shape, or nested structure of `Tensor` shapes,\n",
      "        specifying the event shape(s) of the posterior variables.\n",
      "      bijector: Optional `tfb.Bijector` instance, or nested structure of such\n",
      "        instances, defining support(s) of the posterior variables. The structure\n",
      "        must match that of `event_shape` and may contain `None` values. A\n",
      "        posterior variable will be modeled as\n",
      "        `tfd.TransformedDistribution(underlying_dist, bijector)` if a\n",
      "        corresponding constraining bijector is specified, otherwise it is modeled\n",
      "        as supported on the unconstrained real line.\n",
      "      batch_shape: The `batch_shape` of the output distribution.\n",
      "        Default value: `()`.\n",
      "      base_distribution_cls: Subclass of `tfd.Distribution` that is instantiated\n",
      "        and optionally transformed by the bijector to define the component\n",
      "        distributions. May optionally be a structure of such subclasses\n",
      "        matching `event_shape`.\n",
      "        Default value: `tfd.Normal`.\n",
      "      initial_parameters: Optional `str : Tensor` dictionary specifying initial\n",
      "        values for some or all of the base distribution's trainable parameters,\n",
      "        or a Python `callable` with signature\n",
      "        `value = parameter_init_fn(parameter_name, shape, dtype, seed,\n",
      "        constraining_bijector)`, passed to `tfp.experimental.util.make_trainable`.\n",
      "        May optionally be a structure matching `event_shape` of such dictionaries\n",
      "        and/or callables. Dictionary entries that do not correspond to parameter\n",
      "        names are ignored.\n",
      "        Default value: `{'scale': 1e-2}` (ignored when `base_distribution` does\n",
      "          not have a `scale` parameter).\n",
      "      dtype: Optional float `dtype` for trainable parameters. May\n",
      "        optionally be a structure of such `dtype`s matching `event_shape`.\n",
      "        Default value: `tf.float32`.\n",
      "      validate_args: Python `bool`. Whether to validate input with asserts. This\n",
      "        imposes a runtime cost. If `validate_args` is `False`, and the inputs are\n",
      "        invalid, correct behavior is not guaranteed.\n",
      "        Default value: `False`.\n",
      "      name: Python `str` name prefixed to ops created by this function.\n",
      "        Default value: `None` (i.e., 'build_factored_surrogate_posterior').  seed: PRNG seed; see `tfp.random.sanitize_seed` for details.\n",
      "\n",
      "    Returns:\n",
      "      instance: instance parameterized by trainable `tf.Variable`s.\n",
      "\n",
      "    ### Examples\n",
      "\n",
      "    Consider a Gamma model with unknown parameters, expressed as a joint\n",
      "    Distribution:\n",
      "\n",
      "    ```python\n",
      "    Root = tfd.JointDistributionCoroutine.Root\n",
      "    def model_fn():\n",
      "      concentration = yield Root(tfd.Exponential(1.))\n",
      "      rate = yield Root(tfd.Exponential(1.))\n",
      "      y = yield tfd.Sample(tfd.Gamma(concentration=concentration, rate=rate),\n",
      "                           sample_shape=4)\n",
      "    model = tfd.JointDistributionCoroutine(model_fn)\n",
      "    ```\n",
      "\n",
      "    Let's use variational inference to approximate the posterior over the\n",
      "    data-generating parameters for some observed `y`. We'll build a\n",
      "    surrogate posterior distribution by specifying the shapes of the latent\n",
      "    `rate` and `concentration` parameters, and that both are constrained to\n",
      "    be positive.\n",
      "\n",
      "    ```python\n",
      "    surrogate_posterior = tfp.experimental.vi.build_factored_surrogate_posterior(\n",
      "      event_shape=model.event_shape_tensor()[:-1],  # Omit the observed `y`.\n",
      "      bijector=[tfb.Softplus(),   # Rate is positive.\n",
      "                tfb.Softplus()])  # Concentration is positive.\n",
      "    ```\n",
      "\n",
      "    This creates a trainable joint distribution, defined by variables in\n",
      "    `surrogate_posterior.trainable_variables`. We use `fit_surrogate_posterior`\n",
      "    to fit this distribution by minimizing a divergence to the true posterior.\n",
      "\n",
      "    ```python\n",
      "    y = [0.2, 0.5, 0.3, 0.7]\n",
      "    losses = tfp.vi.fit_surrogate_posterior(\n",
      "      lambda rate, concentration: model.log_prob([rate, concentration, y]),\n",
      "      surrogate_posterior=surrogate_posterior,\n",
      "      num_steps=100,\n",
      "      optimizer=tf_keras.optimizers.Adam(0.1),\n",
      "      sample_size=10)\n",
      "\n",
      "    # After optimization, samples from the surrogate will approximate\n",
      "    # samples from the true posterior.\n",
      "    samples = surrogate_posterior.sample(100)\n",
      "    posterior_mean = [tf.reduce_mean(x) for x in samples]     # mean ~= [1.1, 2.1]\n",
      "    posterior_std = [tf.math.reduce_std(x) for x in samples]  # std  ~= [0.3, 0.8]\n",
      "    ```\n",
      "\n",
      "    If we wanted to initialize the optimization at a specific location, we can\n",
      "    specify initial parameters when we build the surrogate posterior. Note that\n",
      "    these parameterize the distribution(s) over unconstrained values,\n",
      "    so we need to transform our desired constrained locations using the inverse\n",
      "    of the constraining bijector(s).\n",
      "\n",
      "    ```python\n",
      "    surrogate_posterior = tfp.experimental.vi.build_factored_surrogate_posterior(\n",
      "      event_shape=tf.nest.map_fn(tf.shape, initial_loc),\n",
      "      bijector={'concentration': tfb.Softplus(),   # Rate is positive.\n",
      "                'rate': tfb.Softplus()}   # Concentration is positive.\n",
      "      initial_parameters={\n",
      "        'concentration': {'loc': tfb.Softplus().inverse(0.4), 'scale': 1e-2},\n",
      "        'rate': {'loc': tfb.Softplus().inverse(0.2), 'scale': 1e-2}})\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_fit = tfp.experimental.vi.build_factored_surrogate_posterior(\n",
    "    event_shape=(),\n",
    "    bijector=tfb.Sigmoid(),\n",
    ")\n",
    "\n",
    "init_fit = tfp.experimental.vi.build_factored_surrogate_posterior(\n",
    "    event_shape=(),\n",
    "    bijector=tfb.Sigmoid(),\n",
    "    initial_parameters={\n",
    "        'loc': tf.Variable(1.0),\n",
    "        'scale': 0.8,\n",
    "    }\n",
    ")\n",
    "\n",
    "help(tfp.experimental.vi.build_factored_surrogate_posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476c8b5",
   "metadata": {},
   "source": [
    "Let's examine whether the initialized values are in the constrained or unconstrained space.\n",
    "\n",
    "Note: (on line 62-64 of _default_parameter_init_fn in trainable.py of tfp) the scale values get case to float32 which cannot represent 0.01 or 0.8 exactly, so we check that what they return is the same as the floating point representation. Note that they are first being stored as float64, so we have to cast to float64 and then to float32 to check this precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9592ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initialized values by default are 1.9238139390945435, 0.009999997913837433\n",
      "The initialized values when we set loc=1.0 and scale=0.8 are 1.0, 0.8000000715255737\n",
      "Step 1 - Initial value: 0.01\n",
      "Step 2 - After tf.cast: 0.009999999776482582\n",
      "Step 3 - After tf.broadcast_to: 0.009999999776482582\n",
      "Step 4 - Bijector: tfp.bijectors.Softplus(\"softplus\", batch_shape=[], min_event_ndims=0, dtype=float32)\n",
      "Step 5 - After inverse bijector (unconstrained): -4.600178241729736\n",
      "Step 6 - Variable value: -4.600178241729736\n",
      "Step 7 - Variable read: -4.600178241729736\n",
      "Step 8 - After tf.identity: -4.600178241729736\n",
      "Step 9 - After forward bijector (constrained): 0.009999997913837433\n",
      "Step 10 - Normal.scale: 0.009999997913837433\n",
      "Step 11 - Final value (.scale): 0.009999997913837433\n",
      "Step 1 - Initial value: 0.8\n",
      "Step 2 - After tf.cast: 0.800000011920929\n",
      "Step 3 - After tf.broadcast_to: 0.800000011920929\n",
      "Step 4 - Bijector: tfp.bijectors.Softplus(\"softplus\", batch_shape=[], min_event_ndims=0, dtype=float32)\n",
      "Step 5 - After inverse bijector (unconstrained): 0.20338213443756104\n",
      "Step 6 - Variable value: 0.20338213443756104\n",
      "Step 7 - Variable read: 0.20338213443756104\n",
      "Step 8 - After tf.identity: 0.20338213443756104\n",
      "Step 9 - After forward bijector (constrained): 0.8000000715255737\n",
      "Step 10 - Normal.scale: 0.8000000715255737\n",
      "Step 11 - Final value (.scale): 0.8000000715255737\n",
      "Final scale from default initialization after emulating tfp's steps: 0.009999997913837433\n",
      "Final scale from custom initialization after emulating tfp's steps: 0.8000000715255737\n"
     ]
    }
   ],
   "source": [
    "default_initial_values = default_fit.distribution.loc.numpy(), default_fit.distribution.scale.numpy()\n",
    "init_initial_values = init_fit.distribution.loc.numpy(), init_fit.distribution.scale.numpy()\n",
    "print(f\"The initialized values by default are {default_initial_values[0]}, {default_initial_values[1]}\")\n",
    "print(f\"The initialized values when we set loc=1.0 and scale=0.8 are {init_initial_values[0]}, {init_initial_values[1]}\")\n",
    "\n",
    "\n",
    "def pass_through_scale_initialization(initial_value, print_steps=True):\n",
    "    # these are the internal steps that happen when asking for .distribution.scale from a distribution.\n",
    "\n",
    "    \n",
    "    if print_steps:\n",
    "        print(f\"Step 1 - Initial value: {initial_value}\")\n",
    "\n",
    "    # Step 2: Cast to float32 (happens in _default_parameter_init_fn line 62)\n",
    "    # experimental/util/trainable.py line 62\n",
    "    dtype = tf.float32\n",
    "    cast_value = tf.cast(initial_value, dtype=dtype)\n",
    "    if print_steps:\n",
    "        print(f\"Step 2 - After tf.cast: {cast_value.numpy()}\")\n",
    "\n",
    "    # Step 3: Broadcast to shape (happens in _default_parameter_init_fn line 62-64)\n",
    "    # experimental/util/trainable.py line 62-64\n",
    "    shape = ()  # Scalar shape for event_shape=()\n",
    "    broadcast_value = tf.broadcast_to(cast_value, shape)\n",
    "    if print_steps:\n",
    "        print(f\"Step 3 - After tf.broadcast_to: {broadcast_value.numpy()}\")\n",
    "\n",
    "    # Step 4: Get constraining bijector (Normal's scale uses Softplus with low=eps)\n",
    "    # For float32, eps is approximately 1.1920929e-07\n",
    "    # distributions/normal.py lines 153-155 and trainable.py line 168\n",
    "    eps = tf.cast(1.1920929e-07, dtype=dtype)\n",
    "    constraining_bijector = tfb.Softplus(low=eps)\n",
    "    if print_steps:\n",
    "        print(f\"Step 4 - Bijector: {constraining_bijector}\")\n",
    "\n",
    "    # Step 5: Apply inverse bijector to get unconstrained value (happens in _initialize_parameters line 236)\n",
    "    # internal/trainable_state_util.py line 234, 236\n",
    "    unconstrained_value = constraining_bijector.inverse(broadcast_value)\n",
    "    if print_steps:\n",
    "        print(f\"Step 5 - After inverse bijector (unconstrained): {unconstrained_value.numpy()}\")\n",
    "\n",
    "    # Step 6: Create Variable from unconstrained value (happens in as_stateful_builder line 338)\n",
    "    # internal/trainable_state_util.py line 338\n",
    "    variable = tf.Variable(unconstrained_value, name='scale')\n",
    "    if print_steps:\n",
    "        print(f\"Step 6 - Variable value: {variable.numpy()}\")\n",
    "\n",
    "    # Step 7: Read Variable value (happens when accessing .distribution)\n",
    "    # util/deferred_module.py line 152 and trainable_state_util.py line 257\n",
    "    read_value = variable.value()\n",
    "    if print_steps:\n",
    "        print(f\"Step 7 - Variable read: {read_value.numpy()}\")\n",
    "\n",
    "    # Step 8: Apply tf.identity (happens in _apply_parameters line 286)\n",
    "    # internal/trainable_state_util.py line 286\n",
    "    identity_value = tf.identity(read_value)\n",
    "    if print_steps:\n",
    "        print(f\"Step 8 - After tf.identity: {identity_value.numpy()}\")\n",
    "\n",
    "    # Step 9: Apply forward bijector to get constrained value (happens in _apply_parameters line 284)\n",
    "    # internal/trainable_state_util.py line 284\n",
    "    constrained_value = constraining_bijector.forward(identity_value)\n",
    "    if print_steps:\n",
    "        print(f\"Step 9 - After forward bijector (constrained): {constrained_value.numpy()}\")\n",
    "\n",
    "    # Step 10: Pass to Normal constructor and store in _scale\n",
    "    # internal/trainable_state_util.py line 287\n",
    "    normal_dist = tfd.Normal(loc=tf.constant(0.0, dtype=dtype), scale=constrained_value)\n",
    "    if print_steps:\n",
    "        print(f\"Step 10 - Normal.scale: {normal_dist.scale.numpy()}\")\n",
    "\n",
    "    # Step 11: Access via .scale property (returns self._scale)\n",
    "    # distributions/normal.py lines 163-166\n",
    "    final_value = normal_dist.scale\n",
    "    if print_steps:\n",
    "        print(f\"Step 11 - Final value (.scale): {final_value.numpy()}\")\n",
    "    return final_value\n",
    "\n",
    "final_default_scale = pass_through_scale_initialization(1e-2)\n",
    "final_init_scale = pass_through_scale_initialization(0.8)\n",
    "print(f\"Final scale from default initialization after emulating tfp's steps: {final_default_scale.numpy()}\")\n",
    "print(f\"Final scale from custom initialization after emulating tfp's steps: {final_init_scale.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "458fd377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default fit (constrained) empirical mean and std: 0.48628485202789307, 0.002491940511390567\n",
      "Init fit (constrained) empirical mean and std: 0.7071442008018494, 0.15043577551841736\n"
     ]
    }
   ],
   "source": [
    "# empirically observe the mean and std of the samples\n",
    "init_samples = init_fit.sample(100_000)\n",
    "default_samples = default_fit.sample(100_000)\n",
    "print(f\"Default fit (constrained) empirical mean and std: {np.mean(default_samples.numpy())}, {np.std(default_samples.numpy())}\")\n",
    "print(f\"Init fit (constrained) empirical mean and std: {np.mean(init_samples.numpy())}, {np.std(init_samples.numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c85e4",
   "metadata": {},
   "source": [
    "It seems that the values we observe empirically are different from the initialized values, so let's transform the initialized values and see if they match the empirical values. (we use measure transport, i.e. we draw samples from the unconstrained distribution, transform the samples, and then take the empirical mean and std of the transformed samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655383df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default fit empirical constrained mean and std from initial values: 0.4862838387489319, 0.002490274142473936\n",
      "Init fit empirical constrained mean and std from initial values: 0.7078702449798584, 0.14973874390125275\n"
     ]
    }
   ],
   "source": [
    "# sample from the initialized values and transform to constrained space to find empirical constrained mean and std from\n",
    "# initial values\n",
    "init_constrained_samples = tfb.Sigmoid().forward(\n",
    "    tfd.Normal(loc=init_fit.distribution.loc, scale=init_fit.distribution.scale).sample(100_000)\n",
    ")\n",
    "default_constrained_samples = tfb.Sigmoid().forward(\n",
    "    tfd.Normal(loc=default_fit.distribution.loc, scale=default_fit.distribution.scale).sample(100_000)\n",
    ")\n",
    "print(f\"Default fit empirical constrained mean and std from initial values: {np.mean(default_constrained_samples.numpy())}, {np.std(default_constrained_samples.numpy())}\")\n",
    "print(f\"Init fit empirical constrained mean and std from initial values: {np.mean(init_constrained_samples.numpy())}, {np.std(init_constrained_samples.numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26735e55",
   "metadata": {},
   "source": [
    "Since these values match the constrained mean and std above, we conclude that TFP's initialization control changes the underlying unconstrained gaussian distribution in ADVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c0f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfp_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
